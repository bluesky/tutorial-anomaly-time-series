{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "quantitative-parcel",
   "metadata": {},
   "source": [
    "# Anomaly detection in time series data\n",
    "\n",
    "Time series is a sequence of values recorded at *equidistant* time points. Broadly, time can be replaced with pixels, energy(wavelength), temperature, etc.\n",
    "\n",
    "\n",
    "In this tutorial you will learn:\n",
    "\n",
    "* how to explore and compare univariate time series\n",
    "* how to transform data, making it suitable for a model, based on your expert knowledge\n",
    "* unsupervised approach to anomaly detection (elliptical envelope)\n",
    "* supervised binary classification (logistic regression)\n",
    "* how to interpret the models\n",
    "* different metrics for model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absent-marathon",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-cholesterol",
   "metadata": {},
   "source": [
    "## Generate examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "french-album",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import generate_steady_series, generate_anomaly_series\n",
    "\n",
    "numpy.random.seed(1)\n",
    "\n",
    "steady_series = generate_steady_series(N_samples=500, max_size = 1000)\n",
    "anomaly_series = generate_anomaly_series(N_samples=500, max_size = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ready-reward",
   "metadata": {},
   "source": [
    "## Visually explore the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaning-burst",
   "metadata": {},
   "source": [
    "Plot a steady signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mexican-crime",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "fig, ax = plt.subplots(1,2, dpi = 150, figsize = (10, 4))\n",
    "ax[0].plot(steady_series[n].signal)\n",
    "ax[0].set_ylim(0, numpy.max(steady_series[n].signal)*1.05)\n",
    "ax[0].set_xlabel('time')\n",
    "ax[0].set_ylabel('signal')\n",
    "\n",
    "ax[1].plot(steady_series[n].signal[1:]-steady_series[n].signal[:-1])\n",
    "ax[1].set_xlabel('time')\n",
    "ax[1].set_ylabel('signal`s first derivative')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southeast-criticism",
   "metadata": {},
   "source": [
    "Plot an anomalous signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-drunk",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3\n",
    "fig, ax = plt.subplots(1,2, dpi = 150, figsize = (10, 4))\n",
    "ax[0].plot(anomaly_series[n].signal)\n",
    "ax[0].set_ylim(0, numpy.max(anomaly_series[n].signal)*1.05)\n",
    "ax[0].set_xlabel('time')\n",
    "ax[0].set_ylabel('signal')\n",
    "\n",
    "ax[1].plot(anomaly_series[n].signal[1:]-anomaly_series[n].signal[:-1])\n",
    "ax[1].set_xlabel('time')\n",
    "ax[1].set_ylabel('signal`s first derivative')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "substantial-annual",
   "metadata": {},
   "source": [
    "Convert the generated examples to a pandas.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interior-december",
   "metadata": {},
   "outputs": [],
   "source": [
    "steady_df = pandas.DataFrame(steady_series)\n",
    "steady_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wanted-complement",
   "metadata": {},
   "source": [
    "What are the typical length of the `steady` series?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inner-villa",
   "metadata": {},
   "source": [
    "More about Lambda functions: https://www.w3schools.com/python/python_lambda.asp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "right-ethernet",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths_steady = steady_df['signal'].apply(lambda x: len(x))\n",
    "plt.figure(dpi = 100)\n",
    "lengths_steady.hist(bins = 20)\n",
    "plt.xlabel('length of series')\n",
    "plt.ylabel('counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boolean-highlight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # YOUR CODE HERE\n",
    "# anomaly_df = ...\n",
    "\n",
    "# ANSWER\n",
    "anomaly_df = pandas.DataFrame(anomaly_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mighty-cigarette",
   "metadata": {},
   "source": [
    "## Generate features: put your expert knowledge into play"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virgin-victory",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "Think if you need to do any pre-processing on for the data. \n",
    "\n",
    "* Is the mean value important? (saturation, too weak signal)\n",
    "* Is the absolute value of signal variation is important? Can it be that onlt the variation realtive to the mean values are important? (intensity vs peak position)\n",
    "* Are outlier (extreme) values important or they can be ignored?\n",
    "* Are there any NaN values? What values they should be replaced with (if replaced)?\n",
    "\n",
    "Here we chose to center each signal around the mean and normalize by the mean. We ignore 5% of exreme values, such as cosmic rays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opening-hebrew",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import trim_mean\n",
    "from scipy.stats.mstats import trimmed_std\n",
    "from scipy.stats import kurtosis, skew\n",
    "from statsmodels.tsa.stattools import pacf\n",
    "\n",
    "from utils import generate_all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "injured-emerald",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(series, p_cut = 0.025, nan_replace = 0.0): \n",
    "    \n",
    "    # replace NaN values with zeros\n",
    "    series = numpy.nan_to_num(series, nan = nan_replace)\n",
    "    \n",
    "    #center and normalize by the mean\n",
    "    series_mean = trim_mean(series, p_cut)\n",
    "    if series_mean == 0:\n",
    "        return numpy.array([numpy.nan]*len(series))\n",
    "    series = series - series_mean\n",
    "    series = series/series_mean  \n",
    "    \n",
    "    return series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "challenging-violin",
   "metadata": {},
   "source": [
    "How would you modify the preprocessing function if you want to replace NaN values with: \n",
    "* the mean of the series\n",
    "* the median\n",
    "* any unrealistic (e.g. negative) value ? You need to calculate calculate mean before replacing NaN is this case.\n",
    "\n",
    "Apply preprocessing to the signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banned-ceremony",
   "metadata": {},
   "outputs": [],
   "source": [
    "steady_df['scaled_series'] = steady_df['signal'].apply(lambda x: preprocess(x))\n",
    "anomaly_df['scaled_series'] = anomaly_df['signal'].apply(lambda x: preprocess(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hybrid-african",
   "metadata": {},
   "source": [
    "### Explore different aggregative statistics\n",
    "\n",
    "What transformation helps to better separate good and anomalous examples?\n",
    "\n",
    "Explore different options in the function `feature` 2 cells below. A few options to consider:\n",
    "\n",
    "* standard deviation, variance\n",
    "* kurtosis\n",
    "* skew\n",
    "* autocorrelation coefficients, partial autocorrelation coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "third-wedding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# below are a few function you may find useful\n",
    "\n",
    "def trim_series(x, p_cut =0.025):\n",
    "    \"\"\" \n",
    "    Discards p_cut of the smallest and the largets values from the series\n",
    "    Returns:\n",
    "    -------\n",
    "        redcued series\n",
    "    \"\"\"\n",
    "    N = len(x)\n",
    "    N_start = int(p_cut*N)\n",
    "    N_end = int((1-p_cut)*N)\n",
    "    sorted_x = sorted(x)\n",
    "    return sorted_x[N_start:N_end]\n",
    "\n",
    "def autocorr(x, t=1):\n",
    "    \"\"\"calculates autocorrelation coefficient with lag t \"\"\"\n",
    "    return numpy.corrcoef(numpy.array([x[:-t], x[t:]]))[0,1]\n",
    "\n",
    "def trimmed_kurtosis(x):\n",
    "    \"\"\" calculate kurtosis for series without extreme values\"\"\"\n",
    "    trimmed_x = trim_series(x)\n",
    "    return kurtosis(trimmed_x)\n",
    "\n",
    "def trimmed_skew(x):\n",
    "    \"\"\" calculate skew for series without extreme values\"\"\"\n",
    "    trimmed_x = trim_series(x)\n",
    "    return skew(trimmed_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinguished-german",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check different features using the function below\n",
    "# see if the values are different for steady and anomalous series\n",
    "\n",
    "def feature(x):\n",
    "    \n",
    "    #YOUR CODE\n",
    "    \n",
    "    res = autocorr(x,1)\n",
    "    return res\n",
    "\n",
    "plt.figure(dpi = 100)\n",
    "steady_df['scaled_series'].apply(feature).hist(bins = 10, alpha = 0.5, label = 'steady')\n",
    "anomaly_df['scaled_series'].apply(feature).hist(bins = 10, alpha = 0.5, label = 'anomaly')\n",
    "plt.xlabel('feature value')\n",
    "plt.ylabel('counts')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breathing-verification",
   "metadata": {},
   "source": [
    "For now, we generate a set of features based on the functions mentioned above for both the series and its first derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pretty-participation",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_steady = generate_all_features(steady_df['signal'])\n",
    "features_steady['target'] = 1\n",
    "# remove all bad values\n",
    "features_steady = features_steady[~features_steady.isin([numpy.nan,\n",
    "                                                            numpy.inf,\n",
    "                                                            -numpy.inf]).any(1)]\n",
    "\n",
    "\n",
    "features_anomaly = generate_all_features(anomaly_df['signal'])\n",
    "features_anomaly['target'] = -1\n",
    "# remove all bad values\n",
    "features_anomaly = features_anomaly[~features_anomaly.isin([numpy.nan,\n",
    "                                                            numpy.inf,\n",
    "                                                            -numpy.inf]).any(1)]\n",
    "\n",
    "\n",
    "# combine the to data frames into one\n",
    "features = pandas.concat([features_anomaly, features_steady])\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-method",
   "metadata": {},
   "source": [
    "## Unsupervised Model: Elliptical Envelope\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.covariance.EllipticEnvelope.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "human-folks",
   "metadata": {},
   "source": [
    "### Data splitting for unsupervised learning: introducing only good examples during model training\n",
    "\n",
    "We take 90% of the good data to use for training. The rest is used for testing. All of the anomalous data only appear during the testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-embassy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshaffle the data\n",
    "features_steady = features_steady.sample(frac = 1, random_state = 25)\n",
    "\n",
    "#define the split point\n",
    "n_split = int(features_steady.shape[0]*0.9)\n",
    "train_steady = features_steady[:n_split]\n",
    "test_steady = features_steady[n_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-witch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same for anomalous cases\n",
    "# YOUR CODE\n",
    "\n",
    "\n",
    "\n",
    "# ANSWER\n",
    "# reshaffle the data\n",
    "features_anomaly = features_anomaly.sample(frac = 1, random_state = 25)\n",
    "\n",
    "#define the split point\n",
    "n_split = int(features_anomaly.shape[0]*0.0)\n",
    "train_anomaly = features_anomaly[:n_split]\n",
    "test_anomaly = features_anomaly[n_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limiting-purple",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the data and convert pandas.DataFrame object to a numpy.array\n",
    "\n",
    "train = pandas.concat([train_steady, train_anomaly ]).reset_index(drop=True)\n",
    "test = pandas.concat([test_steady, test_anomaly ]).reset_index(drop=True)\n",
    "\n",
    "X_train = train.drop(columns =['target']).values\n",
    "y_train = train['target'].values\n",
    "X_test = test.drop(columns =['target']).values\n",
    "y_test = test['target'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divided-transcript",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# reduce the number of variables to 2\n",
    "\n",
    "scaler =  StandardScaler().fit(X_train) \n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "pca = PCA(2).fit(X_train_scaled)\n",
    "train_pca = pca.transform(X_train_scaled)\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "test_pca = pca.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ready-conjunction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us visualize the data using the two new coordinates\n",
    "plt.figure(dpi = 200, figsize = (4,4))\n",
    "plt.scatter(train_pca[:, 0], train_pca[:, 1], alpha = 0.4, label = 'training set',  edgecolor = 'none')\n",
    "plt.scatter(test_pca[y_test == 1, 0], test_pca[y_test == 1, 1], alpha = 0.4,label = 'test set steady', edgecolor = 'none')\n",
    "plt.scatter(test_pca[y_test == -1, 0], test_pca[y_test == -1, 1], alpha = 0.4,label = 'test set anomaly',edgecolor = 'none')\n",
    "plt.xlabel('PCA1')\n",
    "plt.ylabel('PCA2')\n",
    "plt.xlim((-100, 75))\n",
    "plt.ylim((-50, 100))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sensitive-precipitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine dimensionality reduction and \n",
    "unsup_model = Pipeline([('scaler', StandardScaler()),\n",
    "                 ('pca', PCA(2)),\n",
    "                 ('model', EllipticEnvelope(assume_centered=False,\n",
    "                                            random_state = 50,\n",
    "                                            contamination = 0.0 # change if you think that some anomalies are in the 'steady' data\n",
    "                                           ))])\n",
    "\n",
    "unsup_model.fit(X_train) # notice that no target values (y_train) are needed "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "antique-karaoke",
   "metadata": {},
   "source": [
    "### Evaluating model performance\n",
    "\n",
    "Consider following metricses:\n",
    "* accuracy\n",
    "* confusion matrix (separates tru positive, true negative, false positive and false negative)\n",
    "* recall (what portion of a single class is correctly identified)\n",
    "* precision (how many prediction of a single class are correct)\n",
    "\n",
    "More information at https://en.wikipedia.org/wiki/Confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authorized-motion",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Accuracy is {numpy.round(accuracy_score(y_test, unsup_model.predict(X_test)),2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peaceful-hometown",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, unsup_model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "undefined-theory",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Recall for steady is {numpy.round(recall_score(y_test, unsup_model.predict(X_test)),3)}')\n",
    "print(f'Recall for anomaly is {numpy.round(recall_score(-y_test, -unsup_model.predict(X_test)),3)}')\n",
    "\n",
    "\n",
    "# # YOUR CODE HERE\n",
    "print(f'Precision for steady is {numpy.round(precision_score(y_test, ... ),3)}')\n",
    "print(f'Precision for anomaly is {numpy.round(precision_score(-y_test, ... ),3)}')\n",
    "\n",
    "\n",
    "# # ANSWER\n",
    "# print(f'Precision for steady is {numpy.round(precision_score(y_test, unsup_model.predict(X_test)),3)}')\n",
    "# print(f'Precision for anomaly is {numpy.round(precision_score(-y_test, -unsup_model.predict(X_test)),3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrow-czech",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the envelope\n",
    "from sklearn.covariance import EmpiricalCovariance\n",
    "\n",
    "plt.figure(dpi = 200, figsize = (4,4))\n",
    "emp_cov = EmpiricalCovariance().fit(pca.transform(X_train))\n",
    "xx, yy = numpy.meshgrid(numpy.linspace(-100, 75, 100),\n",
    "                     numpy.linspace(-50, 100, 100))\n",
    "zz = numpy.c_[xx.ravel(), yy.ravel()]\n",
    "mahal_emp_cov = emp_cov.mahalanobis(zz)\n",
    "mahal_emp_cov = mahal_emp_cov.reshape(xx.shape)\n",
    "emp_cov_contour = plt.contour(xx, yy, numpy.sqrt(mahal_emp_cov),\n",
    "                              cmap=plt.cm.PuBu_r, linestyles='dashed', levels = 20)\n",
    "plt.scatter(train_pca[:, 0], train_pca[:, 1], alpha = 0.4, label = 'training set',  edgecolor = 'none')\n",
    "plt.scatter(test_pca[y_test == 1, 0], test_pca[y_test == 1, 1], alpha = 0.4,label = 'test set steady', edgecolor = 'none')\n",
    "plt.scatter(test_pca[y_test == -1, 0], test_pca[y_test == -1, 1], alpha = 0.4,label = 'test set anomaly',edgecolor = 'none')\n",
    "plt.xlim((-100, 75))\n",
    "plt.ylim((-50, 100))\n",
    "plt.xlabel('PCA1')\n",
    "plt.ylabel('PCA2')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "third-ontario",
   "metadata": {},
   "source": [
    "## Supervised Model: Logistic Regression\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "favorite-spirituality",
   "metadata": {},
   "source": [
    "### Data splitting for supervised learning: evenly split the data between the training and the test sets\n",
    "\n",
    "We take 80% of the good and anomalous data to use for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saving-briefing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshaffle the data\n",
    "features_steady = features_steady.sample(frac = 1, random_state = 25)\n",
    "\n",
    "#define the split point\n",
    "n_split = int(features_steady.shape[0]*0.8)\n",
    "train_steady = features_steady[:n_split]\n",
    "test_steady = features_steady[n_split:]\n",
    "\n",
    "# do the same for anomalous cases\n",
    "\n",
    "# reshaffle the data\n",
    "features_anomaly = features_anomaly.sample(frac = 1, random_state = 25)\n",
    "\n",
    "#define the split point\n",
    "n_split = int(features_anomaly.shape[0]*0.8)\n",
    "train_anomaly = features_anomaly[:n_split]\n",
    "test_anomaly = features_anomaly[n_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "different-admission",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the data and convert pandas.DataFrame object to a numpy.array\n",
    "\n",
    "train = pandas.concat([train_steady, train_anomaly ]).reset_index(drop=True)\n",
    "test = pandas.concat([test_steady, test_anomaly ]).reset_index(drop=True)\n",
    "\n",
    "X_train = train.drop(columns =['target']).values\n",
    "y_train = train['target'].values\n",
    "X_test = test.drop(columns =['target']).values\n",
    "y_test = test['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominican-walter",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "sup_model = Pipeline([('scaler', StandardScaler()),\n",
    "                 ('logreg', LogisticRegression(class_weight= 'balanced', random_state=50, C = 100))]) \n",
    "\n",
    "sup_model.fit(X_train, y_train) # both values and targets are needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earned-dominican",
   "metadata": {},
   "source": [
    "### Evaluating model performance\n",
    "\n",
    "Evaluate the model performance according to the same metricses as for the unsuprvised model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advance-significance",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Accuracy is {numpy.round(accuracy_score(y_test, sup_model.predict(X_test)),2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annoying-little",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, sup_model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concerned-bulletin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate precision and recalls for both classes\n",
    "# # YOUR CODE HERE\n",
    "\n",
    "\n",
    "# # ANSWER\n",
    "# print(f'Recall for steady is {numpy.round(recall_score(y_test, sup_model.predict(X_test)),3)}')\n",
    "# print(f'Recall for anomaly is {numpy.round(recall_score(-y_test, -sup_model.predict(X_test)),3)}')\n",
    "\n",
    "# print(f'Precision for steady is {numpy.round(precision_score(y_test, sup_model.predict(X_test)),3)}')\n",
    "# print(f'Precision for anomaly is {numpy.round(precision_score(-y_test, -sup_model.predict(X_test)),3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precious-methodology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can predict probabilities for being steady data\n",
    "sup_model.predict_proba(X_test[:5])[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prepared-scotland",
   "metadata": {},
   "source": [
    "### Interpret the model's weights\n",
    "\n",
    "Model weights can tell about the importance of the corresponding features. A large absolute value of a coefficient means that a small change of the variable leads to a large change of the probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beautiful-qualification",
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = numpy.abs(sup_model['logreg'].coef_).flatten()\n",
    "feature_names = numpy.array(train.drop(columns =['target']).columns)\n",
    "\n",
    "feature_importance = pandas.Series(coefficients, index = feature_names).sort_values(ascending = True)\n",
    "plt.figure(dpi = 150, figsize = (4, 5))\n",
    "feature_importance.plot.barh()\n",
    "plt.title('Feature Importance')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banner-programmer",
   "metadata": {},
   "source": [
    "## Making prediction on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-botswana",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate series\n",
    "# YOUR CODE\n",
    "# s = \n",
    "\n",
    "\n",
    "# EXAMPLE\n",
    "s = numpy.ones(100) + numpy.random.random(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "burning-papua",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "measurement = namedtuple('time_series', 'uid signal')\n",
    "new_series = pandas.DataFrame([measurement('new', s)])\n",
    "\n",
    "# generate features\n",
    "features_new_series = generate_all_features(new_series['signal'])\n",
    "\n",
    "# predict with unsupervised model\n",
    "unsup_pred = unsup_model.predict(features_new_series.values)\n",
    "unsup_pred =('steady' if unsup_pred == 1 else 'anomaly')\n",
    "print(f'Prediction of the unsupervised model is *{unsup_pred}*' )\n",
    "\n",
    "# predict with supervised model\n",
    "sup_pred = sup_model.predict(features_new_series.values)\n",
    "sup_pred =('steady' if sup_pred == 1 else 'anomaly')\n",
    "print(f'Prediction of the unsupervised model is *{sup_pred}*' )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
